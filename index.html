<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-PWVPJ95H');</script>
  <!-- End Google Tag Manager -->

  <meta name="author" content="Minjune Hwang">
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-icon" href="images/favicon.ico">
  <title>Minjune Hwang. Personal Webpage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">

  <!-- Gif toggles -->
  <script>
    function gif_start(image, gif) {
      document.getElementById(image).style.opacity = "1";
      document.getElementById(gif).style.opacity = "0";
    }
    function gif_stop(image, gif) {
      document.getElementById(image).style.opacity = "0";
      document.getElementById(gif).style.opacity = "1";
    }
  </script>
  <!-- Selected publications toggle -->
  <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
  <script type=" text/javascript">
    function pubs_off_start() {
      $('.hidden').hide();
      var e = document.getElementById('sel_pub_button');
      e.classList.add("toggled");
      var e = document.getElementById('all_pub_button');
      e.classList.remove("toggled");
    }
    function pubs_off() {
      $('.hidden').slideUp();
      var e = document.getElementById('sel_pub_button');
      e.classList.add("toggled");
      var e = document.getElementById('all_pub_button');
      e.classList.remove("toggled");
    }
    function pubs_on() {
      $('.hidden').slideDown();
      var e = document.getElementById('all_pub_button');
      e.classList.add("toggled");
      var e = document.getElementById('sel_pub_button');
      e.classList.remove("toggled");
    };
    window.onload = function () {
      pubs_on();
    };
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <!-- Dark mode -->
  <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.5/lib/darkmode-js.min.js"></script>
  <script>
    var options = {
      autoMatchOsTheme: false // default: true
    }

    const darkmode = new Darkmode(options);
    darkmode.showWidget();
  </script>
  <a name="top"></a>
  <!-- body -->
  <div style="max-width:900; width:100%; margin:0 auto">
    <!-- Intro & photo -->
    <div class="table-like" style="width:100%; text-align:left;">
      <div style="flex: 1 0 300px;margin-right:50px">
        <p align="center">
          <name>Minjune Hwang</name>
        </p>
        <p align="center" style="font-family:'Courier New">
          minjuneh at usc dot edu
        </p>
        <p>
          <p>
          Hello! I am a second year PhD Student at USC, where I am advised by <a href="https://danielseita.github.io/" target="_blank" rel="noopener">Prof. Daniel Seita</a> and closely working with <a href="https://ebiyik.github.io/" target="_blank" rel="noopener">Prof. Erdem B&#305y&#305k</a>. 
          </p>
          <p>
          I received a MSCS degree at Stanford, where I worked on robot learning research with <a href="https://profiles.stanford.edu/fei-fei-li"  target="_blank" rel="noopener">Prof. Fei-Fei Li</a> and <a href="https://jiajunwu.com/" target="_blank" rel="noopener">Prof. Jiajun Wu</a>.
          Prior to that I completed my undergraduate studies at UC Berkeley, majoring in CS & Statistics. 
          </p>
          <p>
            Previously, I was an Applied Scientist Intern at <a href="https://amazon.jobs/content/en/teams/ftr/amazon-robotics">Amazon Robotics</a>'s Scanless Tech, where I developed an efficient algorithm that eliminates the need for explicit scanning, with <a href="https://www.frank.preiswerk.com/" target="_blank" rel="noopener">Dr. Frank Preiswerk</a>.
            I have also interned in <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> and <a href="https://apple.com/">Apple</a> (SPG) to solve various problems in robotics.
          </p>
        </p>
        <p align="center">
          <!-- <a href="mailto:minjuneh@usc.edu">Email</a> &nbsp/&nbsp -->
          <a href="data/Minjune_Hwang_CV.pdf">Resume</a> /
          <a href="https://github.com/mj-hwang/">GitHub</a> /
          <a href="https://www.linkedin.com/in/minjune-hwang-751999138/">LinkedIn</a> /
          <a href="https://scholar.google.com/citations?user=juBEoEUAAAAJ&hl=en&oi=ao">Google Scholar</a> 
        </p>
      </div>
      
      <td style="padding:2.5%;width:40%;max-width:40%"></td>
      <div class="image-cropper" style="margin-top:65px;">
        <img src="images/mj_fuji.png" style="width:100%">
      </div>
      </td>
    </div>

    <!--<script>
      // Get the modal
      var modal = document.getElementById('popup-img');

      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('myImg');
      var modalImg = document.getElementById("img01");
      var captionText = document.getElementById("caption");
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = "images/Wechat.jpeg";
        // modalImg.alt = this.alt;
        // captionText.innerHTML = this.alt;
      }

      // Get the <span> element that closes the modal
      var span = document.getElementsByClassName("close")[0];

      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
    </script> -->

    <a name="re"></a>
    <div style="width:100%; text-align:left; margin-top:20; ">
      <heading>Research</heading>
      <a style="font-size: 17px; color: black;">(</a><a href="javascript:pubs_on()" id="all_pub_button" style="font-size:17px">All</a>
      <a style="font-size: 17px; color: black;"> / </a>
      <a href="javascript:pubs_off()" id="sel_pub_button" style="font-size:17px">Highlighted</a><a style="font-size: 17px; color: black;">)</a>
      <div style="margin-top:2px; font-size:13px;">
        (* indicates equal contribution, <sup><span>&#8224;</span></sup> indicates equal advising)
      </div>
      <p style="margin:6px 0;">
        I am interested in developing algorithms that can empower robots that to learn from humans and help daily tasks. My research explores two key, interrelated directions:  
      </p>
      <ul style="margin:4px 0 8px 18px; padding:0;">
        <li><strong>Learing from Human Feedback</strong>: Designing effective algorithms that can reflect a user's true preference and finetuning pretrained policies with various forms of feedback (e.g., demonstrations, preferences, etc) to continuously adapt to user needs.</li>
        <li><strong>Data-Efficient Policy Learning</strong>: Developing methods to train robotic policies in low-data regimes to minimize human effort (e.g., data curation and augmentation) for robust and generalizable policy learning.</li>
      </ul>
      <p style="margin:6px 0;">
        In doing so, I am interested in leveraging natural language. <strong>Language is a powerful medium</strong> for human-robot interaction, essential for clear specification, grounded human-in-the-loop learning, and distilling complex human knowledge into a transferable form.
      </p>
    </div>

    <div style="width:100%; text-align:left; margin-top:10;">
      <table style="width:100%;border:0px;border-spacing:0px 4px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- ReCouPLe -->
        <tr bgcolor="#F3EEB5" class="selected">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/recouple.png' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://hitl-robot-learning.github.io/pdfs/causally.pdf">
            <papertitle>Causally Robust Preference Learning with Reasons</papertitle>
            </a>
            <br>
            <strong><u>Minjune Hwang</u></strong>, 
            Yigit Korkmaz,
            <a class="black_link" href="https://danielseita.github.io/" target="_blank" rel="noopener">Daniel Seita<sup><span>&#8224;</span></sup></a>,
            <a class="black_link" href="https://ebiyik.github.io/" target="_blank" rel="noopener"></a>Erdem B&#305y&#305k<sup><span>&#8224;</span></sup></a>
            <br>
            <em>Under Review</em>, 2025
            <br>
            <em><a href="https://hitl-robot-learning.github.io/">HiTL Workshop</a> @ <strong>RSS 2025</strong> <font color="red"><strong>(Oral)</strong></font></em>
            <br>
            <!-- <a href="https://hitl-robot-learning.github.io/pdfs/causally.pdf">paper</a> |  -->
            <a href="data/posters/ReCouPLe_poster.pdf">poster</a>
            <p></p>
            <p>
            PbRL is widely used for shaping agent behavior to match a user's preference, yet its sparse binary feedback makes it vulnerable to causal confusion.
            We introduce <strong>ReCouPLe</strong>, a lightweight framework that uses natural language rationales to clarify true causal signals behind preference and to improve generalization, by employing orthogonal decomposition.
            </p>
          </td>
        </tr>

        <!-- B3D -->
        <tr class="hidden">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/qdig.png' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2209.08763">
            <papertitle>Red-Teaming Vision-Language-Action Models via Quality Diversity Prompt Generation for Robust Robot Policies</papertitle>
            </a>
            <br>
            Siddharth Srikanth, 
            Freddie Liang, 
            Ya-Chuan Hsu, 
            Varun Bhatt, 
            Shihan Zhao, 
            Henry Chen, 
            <strong><u>Minjune Hwang</u></strong>,
            Akanksha Saran, 
            <a class="black_link" href="https://danielseita.github.io/" target="_blank" rel="noopener">Daniel Seita</a>, 
            Aaquib Tabrez, 
            <a class="black_link" href="https://stefanosnikolaidis.net/" target="_blank" rel="noopener">Stefanos Nikolaidis</a>
            <br>
            <em>Under Review</em>, 2025
            <br>
            <p></p>
            <p>
            We present <strong>Q-DIG</strong>, a novel red-teaming framework for generating diverse task instructions that induce failures in SoTA Vision-Language-Action models using Quality Diversity. We then provide a methodology to leverage these failure modes to improve policies.
            </p>
          </td>
        </tr>
        
        <!-- RedTeamVLA -->
        <tr class="hidden">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/tracking.gif' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2209.08763">
            <papertitle>Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset and Consensus-Based Models</papertitle>
            </a>
            <br>
            Fangyu Wu,
            Dequan Wang,
            <strong><u>Minjune Hwang</u></strong>,
            Chenhui Hao,
            Jiawei Lu,
            Jiamu Zhang,
            Christopher Chou,
            <a class="black_link" href="https://people.eecs.berkeley.edu/~trevor/" target="_blank" rel="noopener">Trevor Darrell</a>,
            <a class="black_link" href="https://bayen.berkeley.edu/alex-bayen" target="_blank" rel="noopener">Alexandre Bayen</a>
            <br>
            <em><strong>ICRA 2025</strong></em>
            <br>
            <em><a href="https://mit-spark.github.io/PAL-ICRA2020/">PAL Workshop</a> @ <strong>ICRA 2020</strong> <font color="red"><strong>(Oral)</strong></font></em>
            <br>
            <a href="https://arxiv.org/abs/2209.08763">arXiv</a> | 
            <a href="https://github.com/b3d-project/b3d">code</a>
            <p></p>
            <p>
            We introduce Berkeley DeepDrive Drone (<strong>B3D</strong>) dataset, a novel traffic dataset and modeling framework tailored to study motion planning in understructured environments. 
            In this project, I was in charge of the object detection and tracking framework for constructing the dataset.
            </p>
          </td>
        </tr>
        
        <!-- NOIR -->
        <tr bgcolor="#F3EEB5" class="selected">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/noir.gif' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://noir-corl.github.io/">
            <papertitle>NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</papertitle>
            </a>
            <br>
            <a class="black_link" href="https://ai.stanford.edu/~zharu/" target="_blank" rel="noopener">Ruohan Zhang*</a>, 
            Sharon Lee*, 
            <strong><u>Minjune Hwang</u>*</strong>, 
            <a class="black_link" href="https://misoshiruseijin.github.io/" target="_blank" rel="noopener">Ayano Hiranaka*</a>, 
            <a class="black_link" href="https://www.chenwangjeremy.net/" target="_blank" rel="noopener">Chen Wang</a>,
            Wensi Ai, 
            Jin Jie Ryan Tan,
            Shreya Gupta, 
            Yilun Hao, 
            Gabrael Levine, 
            Ruohan Gao, 
            Anthony Norcia, 
            <a class="black_link" href="https://profiles.stanford.edu/fei-fei-li" target="_blank" rel="noopener">Li Fei-Fei</a>, 
            <a class="black_link" href="https://jiajunwu.com/" target="_blank" rel="noopener">Jiajun Wu</a>
            <br>
            <em><strong>CoRL 2023</strong></em> 
            <br>
            <em><a href="https://yantianzha.github.io/crl.github.io/">CRL Workshop</a> @ <strong>CoRL 2023</strong> <font color="red"><strong>(Oral)</strong></font></em>
            <br>
            <a href="https://openreview.net/forum?id=eyykI3UIHa">paper</a> | 
            <a href="https://noir-corl.github.io/">project page</a>
            <p></p>
            <p>
            <strong>NOIR</strong> is a general-purpose BRI system that enables humans to command robots to perform everyday activities with their brain signals. 
            Here, I designed a hierarchical EEG decoding system with parameterized skills, as well as a few-shot imitation learning algorithm.
            </p>
          </td>
        </tr>

        <!-- SEED -->
        <tr bgcolor="#F3EEB5" class="selected">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/seed.gif' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://seediros23.github.io/">
            <papertitle>Primitive Skill-Based Robot Learning from Human Evaluative Feedback</papertitle>
            </a>
            <br>
            <strong><u>Minjune Hwang</u>*</strong>, 
            <a class="black_link" href="https://misoshiruseijin.github.io/" target="_blank" rel="noopener">Ayano Hiranaka*</a>, 
            Sharon Lee, 
            <a class="black_link" href="https://www.chenwangjeremy.net/" target="_blank" rel="noopener">Chen Wang</a>, 
            <a class="black_link" href="https://profiles.stanford.edu/fei-fei-li" target="_blank" rel="noopener">Li Fei-Fei</a>, 
            <a class="black_link" href="https://jiajunwu.com/" target="_blank" rel="noopener">Jiajun Wu</a>, 
            <a class="black_link" href="https://ai.stanford.edu/~zharu/" target="_blank" rel="noopener">Ruohan Zhang</a> 
            <br>
            <em><strong>IROS 2023</strong></em> 
            <br>
            <a href="https://arxiv.org/abs/2307.15801">arXiv</a> | 
            <a href="https://seediros23.github.io/">project page</a> |
            <a href="data/posters/SEED_poster.pdf">poster</a>
            <p></p>
            <p>
            We introduce <strong>SEED</strong>, an RLHF algorithm that leverages primitive skills to enable more safe and sample efficient long-horizon task learning. 
            I mainly designed the hierarchical RLHF algorithm and led simulation experiments.
            </p>
          </td>
        </tr>

        <!-- HRON -->
        <tr class="hidden">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/hron.png' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2306.13760">
            <papertitle>Task-Driven Graph Attention for Hierarchical Relational Object Navigation</papertitle>
            </a>
            <br>
            <a class="black_link" href="https://mlingelbach.com/" target="_blank" rel="noopener">Michael Lingelbach</a>, 
            <a class="black_link" href="https://www.chengshuli.me/" target="_blank" rel="noopener">Chengshu Li</a>, 
            <strong><u>Minjune Hwang</u></strong>, 
            Andrey Kurenkov, 
            Alan Lou, 
            <a class="black_link" href="https://robertomartinmartin.com/" target="_blank" rel="noopener">Roberto Martin-Martin</a>, 
            <a class="black_link" href="https://ai.stanford.edu/~zharu/" target="_blank" rel="noopener">Ruohan Zhang</a>, 
            <a class="black_link" href="https://profiles.stanford.edu/fei-fei-li" target="_blank" rel="noopener">Li Fei-Fei</a>, 
            <a class="black_link" href="https://jiajunwu.com/" target="_blank" rel="noopener">Jiajun Wu</a> 
            <br>
            <em><strong>ICRA 2023</strong></em> 
            <br>
            <a href="https://arxiv.org/abs/2306.13760">arXiv</a> | 
            <a href="https://github.com/mjlbach/ssg">github</a>
            <p></p>
            <p>
            We propose a novel solution to object navigation tasks based on a scene graph representation, that combines graph neural networks and task-driven attention for better scalability and learning efficiency. 
            I empirically designed the task-driven graph attention and verified its efficacy in simulation environments.
            </p>
          </td>
        </tr>

        <!-- B1K -->
        <tr bgcolor="#F3EEB5" class="selected">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/behavior.gif' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://behavior.stanford.edu/">
            <papertitle>BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</papertitle>
            </a>
            <br>
            <a class="black_link" href="https://www.chengshuli.me/" target="_blank" rel="noopener">Chengshu Li</a>, 
            Cem Gokmen, 
            Gabrael Levine, 
            <a class="black_link" href="https://robertomartinmartin.com/" target="_blank" rel="noopener">Roberto Martin-Martin</a>, 
            Sanjana Srivastava, 
            <a class="black_link" href="https://www.chenwangjeremy.net/" target="_blank" rel="noopener">Chen Wang</a>, 
            Josiah Wong, 
            <a class="black_link" href="https://ai.stanford.edu/~zharu/" target="_blank" rel="noopener">Ruohan Zhang</a>,
            Michael Lingelbach, 
            Jiankai Sun, 
            Mona Anvari, 
            <strong><u>Minjune Hwang</u></strong>,
            Manasi Sharma, 
            Arman Aydin, 
            Dhruva Bansal, 
            Samuel Hunter, 
            Kyu-Young Kim, 
            Alan Lou, 
            Caleb R Matthews, 
            Ivan Villa-Renteria, 
            Jerry Huayang Tang, 
            Claire Tang, 
            Fei Xia, 
            Silvio Savarese, 
            Hyowon Gweon, 
            Karen Liu, 
            <a class="black_link" href="https://jiajunwu.com/" target="_blank" rel="noopener">Jiajun Wu</a>, 
            <a class="black_link" href="https://profiles.stanford.edu/fei-fei-li" target="_blank" rel="noopener">Li Fei-Fei</a> 
            <br>
            <em><strong>CoRL 2022</strong> <font color="red"><strong>(Best Paper Nomination; Oral)</strong></font></em>
            <br>
            <a href="https://arxiv.org/abs/2403.09227">arXiv</a> | 
            <a href="https://behavior.stanford.edu/">project page</a> |
            <a href="https://github.com/StanfordVL/BEHAVIOR-1K">github</a>
            <p></p>
            <p>
            BEHAVIOR-1K is a benchmark for embodied AI and robotics research with realistic simulation of 1,000 diverse household activities grounded in human needs. 
            As a core developer, I developed generalizable robot kinematic modules and controllers, as well as a foundational library of primitive skills for mobile manipulation.
            </p>
          </td>
        </tr>

        <!-- Minority Reports Defense -->
        <tr class="hidden">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/minority.png' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2004.13799">
            <papertitle>Minority Reports Defense: Defending Against Adversarial Patches</papertitle>
            </a>
            <br>
            <a class="black_link" href="https://www.linkedin.com/in/michael-mccoyd/" target="_blank" rel="noopener">Michael McCoyd</a>, 
            Won Park, 
            Steven Chen, 
            Neil Shah, 
            Ryan Roggenkemper, 
            <strong><u>Minjune Hwang</u></strong>,
            Jason Xinyu Liu, 
            <a class="black_link" href="https://people.eecs.berkeley.edu/~daw/" target="_blank" rel="noopener">David Wagner</a>
            <br>
            <em><strong>ACNS 2020</strong></em> 
            <br>
            <em><a href="https://simla-workshop.github.io/Simla2020/">SiMLA</a> @ <strong>ACNS 2020</strong> <font color="red"><strong>(Best Paper Award)</strong></font></em> 
            <br>
            <a href="https://arxiv.org/abs/2004.13799">arXiv</a> | 
            <a href="https://github.com/mmccoyd/areadefense">github</a>
            <p></p>
            <p>
            We propose a defense against patch attacks based on partially occluding the image around each candidate patch location, so that a few occlusions each completely hide the patch.
            I mainly developed the sparsity-invariant CNNs for adversarial attack detection via occlusion.
            </p>
          </td>
        </tr>

        <!-- Minority Reports Defense -->
        <tr class="hidden">
          <td style="padding:10px;width:25%;vertical-align:middle">
            <img src='images/aihadr1.png' width="200">
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2011.13087">
            <papertitle>Text Analytics for Resilience-Enabled Extreme Events Reconnaissance</papertitle>
            </a>
            <br>
            Alicia Yi-Ting Tsai*,
            Selim Gunay*,
            <strong><u>Minjune Hwang</u>*</strong>,
            Chenglong Li*,
            Pengyuan Zhai*,
            <a class="black_link" href="https://people.eecs.berkeley.edu/~elghaoui/" target="_blank" rel="noopener">Laurent El Ghaoui</a>,
            Khalid M.Mosalam
            <br>
            <em><a href=https://www.hadr.ai/home>AI + HADR</a> @ <strong>NeurIPS 2020 </strong> <font color="red"><strong>(Spotlight)</strong></font></em> 
            <br>
            <a href="https://arxiv.org/abs/2011.13087">arXiv</a> | 
            <a href="https://peer.berkeley.edu/news/peer-research-project-highlight-text-analytics-social-media-resilience-enabled-extreme-events">project page</a> |
            <a href="https://github.com/alicia-tsai/TAR-toolkits">github</a>
            <p></p>
            <p>
            We develop a text analytics framework to extract information from social media data for disaster response. 
            I mainly designed the text analytics framework which used sparse convex optimization and NLP methods.
            </p>
          </td>
        </tr>

      </tbody></table>
    </div>


    <!-- This <i> tag was for some reason necessary for the publication list switch to work -->
    <!-- <i> -->

    <!-- Teaching -->
    <a name="t"></a>
    <div style="width:100%; text-align:left; margin-top:20;">
      <heading>Teaching</heading> <br>
    </div>
    <div style="width:100%; text-align:left; margin-top:10;">
      <li><strong>Graduate Teaching Assistant</strong>: <a href="http://cs231n.stanford.edu/">Stanford CS 231N</a> [2023], Deep Learning for Computer Vision </li>
      <li><strong>Reader</strong> (Undergraduate Teaching Assistant): <a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227BT/">UC Berkeley EE 227BT</a> [2020], Convex Optimization </li>
      <li><strong>Course Instructor</strong>: Ecole Bilingue de Berkeley [2019], Robotics & Programming  (with Prof. Alex Bayen) </li>
      <li><strong>Undergraduate Lab Assistant</strong>: <a href="https://inst.eecs.berkeley.edu/~cs61a/sp18/">UC Berkeley CS 61A</a> [2018],  Structure and Interpretation of Computer Programs </li>
    </div>

    <!-- Industry Experience -->
    <a name="t"></a>
    <div style="width:100%; text-align:left; margin-top:20;">
      <heading>Industry Experience</heading> <br>
    </div>
    <div style="width:100%; text-align:left; margin-top:10;">
      <li><strong>Applied Scientist Intern</strong>, <a href="https://amazon.jobs/en/teams/amazon-robotics/">Amazon Robotics</a></li>
      <li><strong>Research Intern</strong>, <a href="https://www.microsoft.com/en-us/research/">Microsoft</a></li>
      <li><strong>Software Engineering Intern (Motion & Trajectory Planning)</strong>, <a href="https://apple.com/">Apple</a> (SPG)</li>
    </div>

    <!-- Honors & Fellowship -->
    <a name="t"></a>
    <div style="width:100%; text-align:left; margin-top:20;">
      <heading>Honors & Fellowship</heading> <br>
    </div>
    <div style="width:100%; text-align:left; margin-top:10;">
      <li><strong>Viterbi School of Engineering Fellowship</strong> Aug. 2024 - Jul. 2026
      <li><strong>High Distinction in General Scholarship (Magna Cum Laude)</strong> May. 2021
      <li><strong>Summer Undergraduate Research Fellowship (SURF), UC Berkeley</strong> May. 2020
      <li><strong>Berkeley Undergraduate Scholarship</strong> Aug. 2017 - May. 2021
    </div>

    <!-- Service -->
    <a name="s"></a>
    <div style="width:100%; text-align:left; margin-top:20;">
      <heading>Service</heading> <br>
    </div>
    <div style="width:100%; text-align:left; margin-top:10;">
      <li><strong>Serving/Served as a reviewer</strong> for CoRL, ICLR, ICRA, IROS, THRI, and workshops in RSS.</li>
      <li><strong>Mentoring</strong> a number of undegraduate students and summer interns at USC.</li>
      <li><strong>Organizing UROS</strong>, a student-run, cross-department robotics reading group and seminar series at USC.</li>
      <li><strong>Serving as a PhD mentor</strong> for USC CS Undergraduate Mentoring Program.</li>
    </div>

    <div style="width:100%; text-align:left; margin-top:20; display:none">
      <heading>Others </heading>
    </div>
    <div style="max-width:900; width:100%; margin:0 auto; display:none">
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <p align="center">
            <a href="#top"><strong>Home</strong></a>&nbsp&nbsp/&nbsp
            <!-- <a href=/projects><strong>Projects</strong></a>&nbsp&nbsp/&nbsp  -->
            <a href=/posts><strong>Posts</strong></a>
            <!-- <a><strong>Personal (Comming Soon)</strong></a> -->
          </p>
        </tr>
      </table>
    </div>

    <span style="display:none">
      <details>
        <summary>
          Pageviews
        </summary>
        <script type="text/javascript" id="clustrmaps"
          src="//cdn.clustrmaps.com/map_v2.js?cl=5ee2ff&w=400&t=tt&d=5De8UX9TDFsVQrQw4cE3CBhNblYyl2vQbk42qsTB9Fw&co=ffffff&cmo=d99cff&cmn=ffa3b9&ct=808080"></script>
      </details>
    </span>

    <br>
    <hr>
    <div style="margin:auto">
      <center>
        Inspired by <a href="https://jonbarron.info/">this</strong></a> and <a
          href="https://www.jessezhang.net/">this</strong></a>.
      </center>
    </div>
  </div>
</body>

</html>